# 問1

以下の問題に対する答えをyamlファイルに記載してください。

## Section 1

---
### Q1. 以下の空欄に当てはまる言葉をSection1の語彙集から選び、記載せよ。尚、異なる記号の空欄に同じ文言を入れても良い。
<br>

一般に深層学習はデータの前処理->ネットワーク、loss関数及び(ア)の定義->ネットワークの訓練->ネットワークのテスト、チューニングという手順で行われる。
データの前処理として代表的なものとして正規化、標準化の他に、次元削減を行う(イ)や、特徴量同士の無相関化と標準化を行う(ウ)といったものがある。
また、学習の高速化と安定化のため、隠れ層においても正規化が行われる。画像処理では主に(エ)正規化や(オ)正規化が、系統データ処理では主に(カ)正規化が用いられる。
loss関数の代表的なもののうち、(キ)は外れ値に影響を受けやすいため注意が必要である。また、(ク)は分類問題において広くloss関数として用いられる。分類問題のloss関数が最終的な評価指標以外の関数を用いて定義される理由は、loss関数では(ケ)が計算される必要があることによる。
データセットはいくつかに分割される。このうち、ネットワークのチューニングには(コ)を用い、これは(サ)とは独立したものを用いる必要がある。
ネットワークの訓練の際にはバッチサイズ、イテレーション数および学習率を決定する必要がある。バッチサイズを小さくすると(シ)が増加する一方で(ス)が減少する。また、(セ)に囚われにくくなるというメリットもある。学習率を高く設定すると(シ)は減少し、(セ)に囚われにくくなる代わりに(ソ)しなくなるという問題がある。学習を効率的に行うため、(タ)を定義し、学習率を変動させる方法がとられる。
分類問題の評価指標はいくつか考えられるが、例えば二値分類の場合には、入国検査で一人も感染者を見逃したくない場合には(チ)を、検索エンジンにおいて不適合な結果を表示したくない場合は(ツ)を最大化するのがよい。(チ)と(ツ)が同等に重要である場合には正解率や(テ)を用いることとなる。
多値分類の場合、各クラスにおいて評価指標を計算した後に平均を取る(ト)平均や、混合行列全体で集計したTP,FN.FP,TNを用いて評価指標を計算する(ナ)平均が用いられる。これらのうち、(ニ)平均はクラスサイズが考慮された指標となっている。
また、分類問題における閾値の選択には(チ)を縦軸、偽陽性率を横軸としてプロットした(ヌ)が広く用いられる。この曲線によって囲まれる部分の面積を(ネ)といい、この大きさがモデル自体の評価指標として用いられる。

---
### Q2. 以下の空欄に当てはまる言葉をSection1の語彙集から選び、記載せよ。異なる記号の空欄に同じ文言を入れても良い。尚、問題文の後に続く指示に従うこと。
<br>

過学習への対応策として、最も基本的なものとして(ア)や(イ)が挙げられる。これらはそれぞれ損失関数に罰則化項である(ア)項、(イ)項を足し合わせる。この内、(ア)では重みパラメータが0に等しくなるものが出現しやすいため、次元削減手法としての一面も持つ。他の対応策である(ウ)ではネットワークに含まれる一部のユニットの出力を0にすることでモデルのロバストネスを上昇させる。
訓練に用いることができるデータ数が少ない場合、(エ)や(オ)、(カ)といった手法によってそれを補うことができる。特に、(エ)は多様な本番データに対するロバストネスを確保することができる。(オ)では、学習済みモデルの(キ)層以外の重みを固定した上で学習を行う。対して、(カ)では重みを固定せず、全層において学習を行う。(カ)に用いられる代表的なモデルとしては、(ク)や(ケ)が挙げられる。
ラベル不均衡が生じている場合には、(コ)や(サ)、あるいは(シ)を行う。(コ)の代表的な手法として(ス)があり、これは近傍データとの内挿を用いる方法である。
また、ラベルが与えられたデータが少ない一方でラベルが不明なデータが入手可能な場合、(セ)が有効である。

※ ク、ケは語彙集に出現する順序で記載
```
Section1 語彙集

 転移学習 教師あり学習 半教師あり学習 教師なし学習 重み付け 収束 発散 汎化 全結合 再現率 適合率 特異度 分布関数 尤度関数 白色化 学習曲線 勾配 計算時間 局所最適解 メモリ使用量 データ拡張 エポック数増加 訓練データ テストデータ 検証データ プランク面積 k近傍法 t値 p値 F値 ROC曲線 L1正則化 L2正則化 レイヤー バッチ バイアス ジレンマ シグモイド インスタンス プーリング マクロ マイクロ ドロップアウト ファインチューニング オーバーサンプリング アンダーサンプリング ResNet Transformer VGG AlexNet autoencoder SMOTE MSE MAE MSLE BCE MCE PCA TCA ROI AUC optimizer tokenizer scheduler
```
## Section 2

---
### Q1: 以下の空欄に当てはまる言葉を()内で表記が指定されている際にはそれに則り、それ以外の場合には漢字かな表記で記載せよ。
<br>

(ア)はRNNの1種であり、RNNがもつ(イ)や(ウ)といった問題を解消した、長期的な依存関係を学習できるネットワークである。(ア)を特徴づけるものは(エ)であり、ここから捨てる情報を判定する層を(オ)層といい、新たに保存する情報を判定する層を(カ)層という。

※ イ、ウは語彙集に出現する順序で解答

---
### Q2. 以以下の空欄に当てはまる言葉を()内で表記が指定されている際にはそれに則り、それ以外の場合には漢字かな表記で記載せよ。
<br>

ResNetはCNNの層を深くしていくにつれて生じる(ア)や、(イ)といった問題を解消した。ResNetは(ウ)と呼ばれる繰り返し単位で構成されており、これは(エ)層と(オ)との組み合わせで構成される。実戦的には、(カ)の大きさ及びチャンネル数を変化させることで計算効率をよくする、(キ)型の(ウ)が用いられる。

---
### Q3. 以下の空欄に当てはまる言葉を()内で表記が指定されている際にはそれに則り、それ以外の場合には漢字かな表記で記載せよ。
<br>

CNNは画像認識に広く使われるネットワークであり、主に(ア)層、(イ)層、(ウ)層からなる。(ア)層では画像データに対し、二次元配列である(エ)を適用することで(オ)を得る。(イ)層では(オ)を空間的な局所ごとに非線形関数を用いて代表値に集約する事により、ダウンサンプリングを行う。これはニューロンにおける(カ)を大きくすることにあたり、抽象度を一段階高くする役割を持つ。(イ)には様々な関数が用いられるが、現在ではクラス分類のベストプラクティスとして(キ)が用いられる。これは(ア)層からの出力と(イ)層からの出力を同じサイズとしており、ネットワークの最後に(ク)層を利用しないため、パラメータ数を大きく削減し、過学習の抑制及び計算量の削減が達成できる。

### Q4. 以下の空欄に当てはまる言葉を()内で表記が指定されている際にはそれに則り、それ以外の場合には漢字かな表記で記載せよ。
<br>

(ア)では下の式に示される(イ)を用いる。この式の中で、QとKの内積計算が行われているが、これはQとKの(ウ)を計測することにあたる。(ア)では(イ)を用いるため、入力データを(エ)ベクトルにする必要がある。この処理には(オ)が用いられ、(カ)及び(キ)といった処理が含まれる。また、(ア)では埋め込み処理が行われることが多い。この内、[(ク)]トークンは文頭に置かれ、主に(ケ)タスクにおいて良い成績を残している。また、(コ)は言語の特性である並進不変性、単調性、対称性といった情報を扱えるため、有用である。

![Attention(Q,K,V) &= softmax(\frac{QK^T}{\sqrt{d_k}})V
](https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblack%7D%5Cdisplaystyle+Attention%28Q%2CK%2CV%29+%26%3D+softmax%28%5Cfrac%7BQK%5ET%7D%7B%5Csqrt%7Bd_k%7D%7D%29V%0A)

※ カ、キは語彙集に出現する順序で解答

```
Section2 語彙集

分類 回帰 静止電位 受容野 発火頻度 勾配消失 勾配爆発 隠れ状態 畳み込み 全結合 関連性 固定長 エラー検出 残差ブロック 忘却ゲート 入力ゲート モデル劣化 シナプス強度 特徴マップ セル状態 スキップ接続 ボトルネック プーリング カーネル CLS UNK SEP MASK Tokenizer padding truncation AlexNet VGG LSTM Transformer

以下は一行を一つの選択肢として記載する。
Self Attention
Scaled Dot-Product Attention
Multihead Attention
Positional Embedding
Character-based Embedding
Word Embedding
Max Pooling
Average Pooling
Global Average Pooling
Global Max Pooling
```
